{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv3 Train Sageband.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeqinghuang516/UCSD-ECE285-Object-Detection-Using-Deep-Learning/blob/master/YOLO/YOLOv3_Train_Sageband.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10sl9S8OcyMC",
        "colab_type": "text"
      },
      "source": [
        "## Git clone the repository and install the requirments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKpNQ-lTbpN1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "819cd8fb-c9de-4bbb-84d6-47ecda5b0afc"
      },
      "source": [
        "! git clone https://github.com/yeqinghuang516/UCSD-ECE285-Object-Detection-Using-Deep-Learning.git\n",
        "% cd UCSD-ECE285-Object-Detection-Using-Deep-Learning/YOLO\n",
        "! sudo pip3 install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'UCSD-ECE285-Object-Detection-Using-Deep-Learning'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 19 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n",
            "/content/UCSD-ECE285-Object-Detection-Using-Deep-Learning/YOLO\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.16.4)\n",
            "Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.0.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.13.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.13.1)\n",
            "Collecting terminaltables (from -r requirements.txt (line 7))\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (4.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->-r requirements.txt (line 3)) (1.12.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.13.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.8.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 6)) (0.15.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->-r requirements.txt (line 8)) (0.46)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->-r requirements.txt (line 4)) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow->-r requirements.txt (line 5)) (2.8.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->-r requirements.txt (line 5)) (3.0.5)\n",
            "Building wheels for collected packages: terminaltables\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built terminaltables\n",
            "Installing collected packages: terminaltables\n",
            "Successfully installed terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b5i2Ha4cD5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "sys.path.append('/content/UCSD-ECE285-Object-Detection-Using-Deep-Learning/YOLO/')\n",
        "\n",
        "\n",
        "from models import *\n",
        "from utils.utils import *\n",
        "from utils.logger import *\n",
        "from utils.datasets import *\n",
        "from mytest import evaluate\n",
        "\n",
        "from terminaltables import AsciiTable\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import torchvision as tv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6bKF6vHfCWe",
        "colab_type": "text"
      },
      "source": [
        "## Setting up parameters for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcBO9GqLcxaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 200 # number of epochs\n",
        "batch_size = 32 #size of each image batch\n",
        "gradient_accumulations = 2 #number of gradient accums before step\n",
        "model_def = 'config/yolov3.cfg' #path to model definition file\n",
        "n_cpu = 8 #number of cpu threads to use during batch generation\n",
        "img_size = 224#size of each image dimension\n",
        "checkpoint_interval = 1 # interval between saving model weights\n",
        "evaluation_interval = 2 # interval evaluations on validation set\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger = Logger(\"logs\")\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "metrics = ['grid_size', 'loss', 'x', 'y', 'w', 'h', 'conf', 'cls', 'cls_acc', 'recall50', 'recall75', 'precision', 'conf_obj', 'conf_noobj']\n",
        "class_names = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba_B6wRufIHc",
        "colab_type": "text"
      },
      "source": [
        "## Download and Initialize datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTXOCpYucxca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "aa861ba8-ec3e-4d6e-b670-996a4e2c12ed"
      },
      "source": [
        "os.makedirs('data', exist_ok = True)\n",
        "root = 'data'\n",
        "VOCdataset = VOCDataset(root, image_set = 'train', train = True)\n",
        "VOCtestset = VOCDataset(root, image_set = 'val')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 90112/1999639040 [00:00<40:35, 821044.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "download =  True\n",
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to data/VOCtrainval_11-May-2012.tar\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999642624it [00:40, 81777583.64it/s]                                "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "download =  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7foox8HXf-rN",
        "colab_type": "text"
      },
      "source": [
        "## Define pretrained weights (if training from previous checkpoints or pretrained weights)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NM6ecejgEuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_weights = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B6brlIDgo6t",
        "colab_type": "text"
      },
      "source": [
        "## Initiate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr6X7SSqcxei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2dfaae88-4a50-4ac9-ba0b-decfced066ba"
      },
      "source": [
        "model = Darknet(model_def, loss_mode = \"modified\").to(device)\n",
        "model.apply(weights_init_normal);\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
        "cur_epoch = 0\n",
        "\n",
        "# If specified we start from checkpoint, load checkpoint\n",
        "if pretrained_weights:\n",
        "    if pretrained_weights.endswith(\".pth\"):\n",
        "        checkpoint = torch.load(pretrained_weights)\n",
        "        model.load_state_dict(checkpoint['net'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        cur_epoch = checkpoint['epoch'] + 1\n",
        "        print('load state dict')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The loss mode is modified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhOdEJuAgwWW",
        "colab_type": "text"
      },
      "source": [
        "## Initiate Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6qBQO0tcxi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset = VOCdataset,\n",
        "    batch_size= batch_size,\n",
        "    num_workers= n_cpu,\n",
        "    shuffle = True,\n",
        "    pin_memory= True,\n",
        "    drop_last = False,\n",
        "    collate_fn= VOCdataset.collate_fn,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K01wV5QqhRnt",
        "colab_type": "text"
      },
      "source": [
        "## Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVFPY2uJcxlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(cur_epoch, epochs):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "    for batch_i, (imgs, targets) in enumerate(dataloader):\n",
        "      batches_done = len(dataloader) * epoch + batch_i\n",
        "      imgs = Variable(imgs.to(device))\n",
        "      targets = Variable(targets.to(device), requires_grad=False)\n",
        "\n",
        "      loss, outputs = model(imgs, targets)\n",
        "      loss.backward()\n",
        "\n",
        "      if batches_done % gradient_accumulations:\n",
        "          # Accumulates gradient before each step\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "    \n",
        "      # ----------------\n",
        "      #   Log progress\n",
        "      # ----------------\n",
        "\n",
        "      log_str = \"\\n---- [Epoch %d/%d, Batch %d/%d] ----\\n\" % (epoch, epochs, batch_i, len(dataloader))\n",
        "\n",
        "      metric_table = [[\"Metrics\", *[f\"YOLO Layer {i}\" for i in range(len(model.yolo_layers))]]]\n",
        "\n",
        "      # Log metrics at each YOLO layer\n",
        "      for i, metric in enumerate(metrics):\n",
        "          formats = {m: \"%.6f\" for m in metrics}\n",
        "          formats[\"grid_size\"] = \"%2d\"\n",
        "          formats[\"cls_acc\"] = \"%.2f%%\"\n",
        "          row_metrics = [formats[metric] % yolo.metrics.get(metric, 0) for yolo in model.yolo_layers]\n",
        "          metric_table += [[metric, *row_metrics]]\n",
        "\n",
        "          # Tensorboard logging\n",
        "          tensorboard_log = []\n",
        "          for j, yolo in enumerate(model.yolo_layers):\n",
        "              for name, metric in yolo.metrics.items():\n",
        "                  if name != \"grid_size\":\n",
        "                      tensorboard_log += [(f\"{name}_{j+1}\", metric)]\n",
        "          tensorboard_log += [(\"loss\", loss.item())]\n",
        "          logger.list_of_scalars_summary(tensorboard_log, batches_done)\n",
        "\n",
        "      log_str += AsciiTable(metric_table).table\n",
        "      log_str += f\"\\nTotal loss {loss.item()}\"\n",
        "\n",
        "      # Determine approximate time left for epoch\n",
        "      epoch_batches_left = len(dataloader) - (batch_i + 1)\n",
        "      time_left = datetime.timedelta(seconds=epoch_batches_left * (time.time() - start_time) / (batch_i + 1))\n",
        "      log_str += f\"\\n---- ETA {time_left}\"\n",
        "      \n",
        "      # print training log\n",
        "      print(log_str)\n",
        "\n",
        "      model.seen += imgs.size(0)\n",
        "    \n",
        "    if epoch % evaluation_interval == 0:\n",
        "        print(\"\\n---- Evaluating Model ----\")\n",
        "        # Evaluate the model on the validation set\n",
        "        precision, recall, AP, f1, ap_class = evaluate(\n",
        "            model,\n",
        "            dataset = VOCtestset,\n",
        "            iou_thres=0.5,\n",
        "            conf_thres=0.5,\n",
        "            nms_thres=0.5,\n",
        "            img_size=img_size,\n",
        "            batch_size=8,\n",
        "        )\n",
        "        evaluation_metrics = [\n",
        "            (\"val_precision\", precision.mean()),\n",
        "            (\"val_recall\", recall.mean()),\n",
        "            (\"val_mAP\", AP.mean()),\n",
        "            (\"val_f1\", f1.mean()),\n",
        "        ]\n",
        "        logger.list_of_scalars_summary(evaluation_metrics, epoch)\n",
        "\n",
        "        # Print class APs and mAP\n",
        "        ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
        "        for i, c in enumerate(ap_class):\n",
        "            ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
        "        print(AsciiTable(ap_table).table)\n",
        "        print(f\"---- mAP {AP.mean()}\")\n",
        "    \n",
        "    # save checkpoints\n",
        "    if epoch % checkpoint_interval == 0:\n",
        "        state_dict = {'net': model.state_dict(), 'optimizer': optimizer.state_dict(),'epoch': epoch}\n",
        "        torch.save(state_dict, f\"checkpoints/yolov3_ckpt_%d.pth\" % epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHb0GvYgcxni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWHYIyCWcxpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsBgoQU-cxsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIkl6eRhcxue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbNisAo1cxxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMm6YoxXcxzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfYuygnmcx1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}